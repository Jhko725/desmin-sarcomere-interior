from pathlib import Path

import numpy as np
import pandas as pd


## For manipulating file paths
STORM_filepaths = {
    "desmin_alphaactinin_600nm": "2017-01-23-10-37-54/particles.csv",
    "desmin_alphaactinin_1.2um": "2020-08-25-12-24-36 sample2a/particles.dat",
    "actin_desmin_600nm": "2017-03-21-11-31-56/particles.csv",
    "actin_desmin_1.2um": "2020-07-28-12-00-44 sample1c desmin568 actin647/particles.dat",
    "desmin_actin_2.5um": "2020-07-28-11-45-42 sample1b desmin568 actin647/particles.dat",
}


def fetch_STORM_experiment_filepath(experiment_name, STORM_data_dir):
    assert experiment_name in STORM_filepaths.keys(), (
        "Experiment with specified name does not exist!"
    )
    return Path(STORM_data_dir).joinpath(STORM_filepaths[experiment_name])


## For reading raw data files produced by Bruker Vutara352 microscope
def read_Vutara352_localization_data(filepath):
    path_obj = Path(filepath)
    file_ext = path_obj.suffix

    if file_ext == ".csv":
        localization_dataframe = _read_particles_csv(path_obj)
    elif file_ext == ".dat":
        localization_dataframe = _read_particles_dat(path_obj)
    else:
        raise ValueError("Vutara output file is either in .dat or .csv format!")

    print(localization_dataframe.info())
    return localization_dataframe


def _read_particles_csv(path_csv):
    particles_dataframe = pd.read_csv(path_csv, header=0)
    return particles_dataframe


def _read_particles_dat(path_dat):
    header_size = 572  # This was also reverse-engineered by manually decoding the particles.dat bytecode,
    # that is, by counting the number of bytes until the start of data was reached
    # Judging from Bruker's documentation, depending on what you do with Vutara before exporting(generate extra columns), this number could increase
    encoding_dtype = _get_particle_dat_encoding_datatype()

    with path_dat.open(mode="rb") as binary_file:
        header = binary_file.read(header_size)
        data = np.fromfile(binary_file, dtype=encoding_dtype)

    particles_dataframe = pd.DataFrame.from_records(data)
    return particles_dataframe


def _get_particle_dat_encoding_datatype():
    """Returns the composite datatype that is used to encode the particles.dat files generated by newer versions of the Vutara software.
    This datatype was reversed-engineered through consulting Bruker's documentation for Vutara's file format, manually decoding the particles.dat bytecode
    , and comparing this to particles.csv produced by older versions of the Vutara software."""

    integer_type = "<i4"
    float_type = "<f8"
    binary_type = "?"

    field_encoding_list = [
        ("image-ID", integer_type),
        ("time-point", integer_type),
        ("cycle", integer_type),
        ("z-step", integer_type),
        ("frame", integer_type),
        ("accum", integer_type),
        ("probe", integer_type),
        ("photon-count", float_type),
        ("photon-count11", float_type),
        ("photon-count12", float_type),
        ("photon-count21", float_type),
        ("photon-count22", float_type),
        ("psfx", float_type),
        ("psfy", float_type),
        ("psfz", float_type),
        ("psf-photon-count", float_type),
        ("x", float_type),
        ("y", float_type),
        ("z", float_type),
        ("stdev", float_type),
        ("amp", float_type),
        ("background11", float_type),
        ("background12", float_type),
        ("background21", float_type),
        ("background22", float_type),
        ("maxResidualSlope", float_type),
        ("chisq", float_type),
        ("log-likelihood", float_type),
        ("llr", float_type),
        ("accuracy", float_type),
        ("precisionx", float_type),
        ("precisiony", float_type),
        ("precisionz", float_type),
        ("fiducial", binary_type),
        ("valid", binary_type),
    ]

    return np.dtype(field_encoding_list)


## For preprocessing the dataframe produced by the read functions above
def drop_invalid_localizations(localization_dataframe):
    valid_inds = localization_dataframe["valid"] == True
    return localization_dataframe[valid_inds]


def get_unique_probe_ids(localization_dataframe):
    return np.sort(pd.unique(localization_dataframe["probe"]))


def get_per_probe_localizations(localization_dataframe):
    probe_ids = get_unique_probe_ids(localization_dataframe)
    localization_dfs = [
        get_single_probe_localizations(localization_dataframe, id) for id in probe_ids
    ]
    return probe_ids, localization_dfs


def get_single_probe_localizations(localization_dataframe, probe_id):
    selected_inds = (localization_dataframe["photon-count"] > 0) & (
        localization_dataframe["probe"] == probe_id
    )
    return localization_dataframe[selected_inds]
